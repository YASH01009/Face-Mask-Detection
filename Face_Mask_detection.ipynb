{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face_Mask_detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym0JalvPhPRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from imutils import paths\n",
        "\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "import os"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaTvFctOhZas",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "065b0d71-1d1e-4119-f261-2c49d592152d"
      },
      "source": [
        "# load the VGG16 network and initialize the label encoder\n",
        "print(\"[INFO] loading network...\")\n",
        "model = VGG16(weights=\"imagenet\", include_top=False)\n",
        "le = None"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading network...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzTlgqIZhuqa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2bb875c3-83b9-409a-d8c6-ca4e43c30fab"
      },
      "source": [
        "# mount drive to the notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LuNfRgtisJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_PATH = '/content/drive/My Drive/Face_Mask_Dataset'\n",
        "\n",
        "TRAIN = 'Training'\n",
        "TEST = 'Testing'\n",
        "VAL = 'Validation'\n",
        "\n",
        "MASK = '/WithMask'\n",
        "NO_MASK = '/WithoutMask'\n",
        "\n",
        "BASE_CSV_PATH = '/content/drive/My Drive/Face_Mask_Dataset/Output'\n",
        "LE_PATH = os.path.sep.join([BASE_CSV_PATH, \"le.cpickle\"])\n",
        "MODEL_PATH = os.path.sep.join([BASE_CSV_PATH, \"model.cpickle\"])\n",
        "\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSD4hPC4zDyg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dfc06f8e-624c-4748-fe7c-8f03d6bc5555"
      },
      "source": [
        "# load the VGG16 network and initialize the label encoder\n",
        "print(\"[INFO] loading network...\")\n",
        "model = VGG16(weights=\"imagenet\", include_top=False)\n",
        "le = None"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading network...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zXSpoqLyDjT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "99292f74-d543-4849-f919-7d130b8d6ff2"
      },
      "source": [
        "# loop over the data splits\n",
        "for split in (VAL, TRAIN):\n",
        "  # grab all image paths in the current split\n",
        "  print(\"[INFO] processing '{} split'...\".format(split))\n",
        "  p = os.path.sep.join([BASE_PATH, split])\n",
        "  imagePaths = list(paths.list_images(p))\n",
        "  \n",
        "  # randomly shuffle the image paths and then extract the class\n",
        "  # labels from the file paths\n",
        "  random.shuffle(imagePaths)\n",
        "  labels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
        "\n",
        "\t# if the label encoder is None, create it\n",
        "  if le is None:\n",
        "    le = LabelEncoder()\n",
        "    le.fit(labels)\n",
        "\n",
        "\t# open the output CSV file for writing\n",
        "  csvPath = os.path.sep.join([BASE_CSV_PATH, \"{}.csv\".format(split)])\n",
        "  csv = open(csvPath, \"w\")\n",
        "\n",
        "\t# loop over the images in batches\n",
        "  for (b, i) in enumerate(range(0, len(imagePaths), BATCH_SIZE)):\n",
        "\t\t# extract the batch of images and labels, then initialize the\n",
        "\t\t# list of actual images that will be passed through the network\n",
        "\t\t# for feature extraction\n",
        "    print(\"[INFO] processing batch {}/{}\".format(b + 1, int(np.ceil(len(imagePaths) / float(BATCH_SIZE)))))\n",
        "    batchPaths = imagePaths[i:i + BATCH_SIZE]\n",
        "    batchLabels = le.transform(labels[i:i + BATCH_SIZE])\n",
        "    batchImages = []\n",
        "\n",
        "\t\t# loop over the images and labels in the current batch\n",
        "    for imagePath in batchPaths:\n",
        "\t\t\t# load the input image using the Keras helper utility\n",
        "\t\t\t# while ensuring the image is resized to 224x224 pixels\n",
        "      image = load_img(imagePath, target_size=(224, 224))\n",
        "      image = img_to_array(image)\n",
        "\n",
        "\t\t\t# preprocess the image by (1) expanding the dimensions and\n",
        "\t\t\t# (2) subtracting the mean RGB pixel intensity from the\n",
        "\t\t\t# ImageNet dataset\n",
        "      image = np.expand_dims(image, axis=0)\n",
        "      image = preprocess_input(image)\n",
        "\n",
        "\t\t\t# add the image to the batch\n",
        "      batchImages.append(image)\n",
        "\n",
        "\t\t# pass the images through the network and use the outputs as\n",
        "\t\t# our actual features, then reshape the features into a\n",
        "\t\t# flattened volume\n",
        "    batchImages = np.vstack(batchImages)\n",
        "    features = model.predict(batchImages, batch_size=BATCH_SIZE)\n",
        "    features = features.reshape((features.shape[0], 7 * 7 * 512))\n",
        "\n",
        "\t\t# loop over the class labels and extracted features\n",
        "    for (label, vec) in zip(batchLabels, features):\n",
        "\t\t\t# construct a row that exists of the class label and\n",
        "\t\t\t# extracted features\n",
        "      vec = \",\".join([str(v) for v in vec])\n",
        "      csv.write(\"{},{}\\n\".format(label, vec))\n",
        "\n",
        "\t# close the CSV file\n",
        "  csv.close()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] processing 'Validation split'...\n",
            "[INFO] processing batch 1/45\n",
            "[INFO] processing batch 2/45\n",
            "[INFO] processing batch 3/45\n",
            "[INFO] processing batch 4/45\n",
            "[INFO] processing batch 5/45\n",
            "[INFO] processing batch 6/45\n",
            "[INFO] processing batch 7/45\n",
            "[INFO] processing batch 8/45\n",
            "[INFO] processing batch 9/45\n",
            "[INFO] processing batch 10/45\n",
            "[INFO] processing batch 11/45\n",
            "[INFO] processing batch 12/45\n",
            "[INFO] processing batch 13/45\n",
            "[INFO] processing batch 14/45\n",
            "[INFO] processing batch 15/45\n",
            "[INFO] processing batch 16/45\n",
            "[INFO] processing batch 17/45\n",
            "[INFO] processing batch 18/45\n",
            "[INFO] processing batch 19/45\n",
            "[INFO] processing batch 20/45\n",
            "[INFO] processing batch 21/45\n",
            "[INFO] processing batch 22/45\n",
            "[INFO] processing batch 23/45\n",
            "[INFO] processing batch 24/45\n",
            "[INFO] processing batch 25/45\n",
            "[INFO] processing batch 26/45\n",
            "[INFO] processing batch 27/45\n",
            "[INFO] processing batch 28/45\n",
            "[INFO] processing batch 29/45\n",
            "[INFO] processing batch 30/45\n",
            "[INFO] processing batch 31/45\n",
            "[INFO] processing batch 32/45\n",
            "[INFO] processing batch 33/45\n",
            "[INFO] processing batch 34/45\n",
            "[INFO] processing batch 35/45\n",
            "[INFO] processing batch 36/45\n",
            "[INFO] processing batch 37/45\n",
            "[INFO] processing batch 38/45\n",
            "[INFO] processing batch 39/45\n",
            "[INFO] processing batch 40/45\n",
            "[INFO] processing batch 41/45\n",
            "[INFO] processing batch 42/45\n",
            "[INFO] processing batch 43/45\n",
            "[INFO] processing batch 44/45\n",
            "[INFO] processing batch 45/45\n",
            "[INFO] processing 'Training split'...\n",
            "[INFO] processing batch 1/65\n",
            "[INFO] processing batch 2/65\n",
            "[INFO] processing batch 3/65\n",
            "[INFO] processing batch 4/65\n",
            "[INFO] processing batch 5/65\n",
            "[INFO] processing batch 6/65\n",
            "[INFO] processing batch 7/65\n",
            "[INFO] processing batch 8/65\n",
            "[INFO] processing batch 9/65\n",
            "[INFO] processing batch 10/65\n",
            "[INFO] processing batch 11/65\n",
            "[INFO] processing batch 12/65\n",
            "[INFO] processing batch 13/65\n",
            "[INFO] processing batch 14/65\n",
            "[INFO] processing batch 15/65\n",
            "[INFO] processing batch 16/65\n",
            "[INFO] processing batch 17/65\n",
            "[INFO] processing batch 18/65\n",
            "[INFO] processing batch 19/65\n",
            "[INFO] processing batch 20/65\n",
            "[INFO] processing batch 21/65\n",
            "[INFO] processing batch 22/65\n",
            "[INFO] processing batch 23/65\n",
            "[INFO] processing batch 24/65\n",
            "[INFO] processing batch 25/65\n",
            "[INFO] processing batch 26/65\n",
            "[INFO] processing batch 27/65\n",
            "[INFO] processing batch 28/65\n",
            "[INFO] processing batch 29/65\n",
            "[INFO] processing batch 30/65\n",
            "[INFO] processing batch 31/65\n",
            "[INFO] processing batch 32/65\n",
            "[INFO] processing batch 33/65\n",
            "[INFO] processing batch 34/65\n",
            "[INFO] processing batch 35/65\n",
            "[INFO] processing batch 36/65\n",
            "[INFO] processing batch 37/65\n",
            "[INFO] processing batch 38/65\n",
            "[INFO] processing batch 39/65\n",
            "[INFO] processing batch 40/65\n",
            "[INFO] processing batch 41/65\n",
            "[INFO] processing batch 42/65\n",
            "[INFO] processing batch 43/65\n",
            "[INFO] processing batch 44/65\n",
            "[INFO] processing batch 45/65\n",
            "[INFO] processing batch 46/65\n",
            "[INFO] processing batch 47/65\n",
            "[INFO] processing batch 48/65\n",
            "[INFO] processing batch 49/65\n",
            "[INFO] processing batch 50/65\n",
            "[INFO] processing batch 51/65\n",
            "[INFO] processing batch 52/65\n",
            "[INFO] processing batch 53/65\n",
            "[INFO] processing batch 54/65\n",
            "[INFO] processing batch 55/65\n",
            "[INFO] processing batch 56/65\n",
            "[INFO] processing batch 57/65\n",
            "[INFO] processing batch 58/65\n",
            "[INFO] processing batch 59/65\n",
            "[INFO] processing batch 60/65\n",
            "[INFO] processing batch 61/65\n",
            "[INFO] processing batch 62/65\n",
            "[INFO] processing batch 63/65\n",
            "[INFO] processing batch 64/65\n",
            "[INFO] processing batch 65/65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O67l_bZzJrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize the label encoder to disk\n",
        "f = open(LE_PATH, \"wb\")\n",
        "f.write(pickle.dumps(le))\n",
        "f.close()"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfMK9PdI1WkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def load_data_split(splitPath):\n",
        "\t# initialize the data and labels\n",
        "\tdata = []\n",
        "\tlabels = []\n",
        "\n",
        "\t# loop over the rows in the data split file\n",
        "\tfor row in open(splitPath):\n",
        "\t\t# extract the class label and features from the row\n",
        "\t\trow = row.strip().split(\",\")\n",
        "\t\tlabel = row[0]\n",
        "\t\tfeatures = np.array(row[1:], dtype=\"float\")\n",
        "\n",
        "\t\t# update the data and label lists\n",
        "\t\tdata.append(features)\n",
        "\t\tlabels.append(label)\n",
        "\n",
        "\t# convert the data and labels to NumPy arrays\n",
        "\tdata = np.array(data)\n",
        "\tlabels = np.array(labels)\n",
        "\n",
        "\t# return a tuple of the data and labels\n",
        "\treturn (data, labels)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP16rtfy2f1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# derive the paths to the training and testing CSV files\n",
        "trainingPath = os.path.sep.join([BASE_CSV_PATH, \"{}.csv\".format(TRAIN)])\n",
        "testingPath = os.path.sep.join([BASE_CSV_PATH, \"{}.csv\".format(VAL)])"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pC6b_u12kNg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72d9dc2d-b7d0-4636-abfa-b02a1c4a7ad7"
      },
      "source": [
        "# load the data from disk\n",
        "print(\"[INFO] loading data...\")\n",
        "(trainX, trainY) = load_data_split(trainingPath)\n",
        "(testX, testY) = load_data_split(testingPath)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yMdxqpa203W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the label encoder from disk\n",
        "le = pickle.loads(open(LE_PATH, \"rb\").read())"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6hSN3XD2oyH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "6714ca21-f986-4d6f-d1a5-9ac02a6ce744"
      },
      "source": [
        "# train the model\n",
        "print(\"[INFO] training model...\")\n",
        "model = LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\", max_iter=150)\n",
        "model.fit(trainX, trainY)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] training model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=150,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyWEgJ842sP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "fe84a65b-8da2-4813-f123-a609b9696c15"
      },
      "source": [
        "# evaluate the model\n",
        "print(\"[INFO] evaluating...\")\n",
        "preds = model.predict(testX)\n",
        "print(classification_report(testY, preds, target_names=le.classes_))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] evaluating...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    WithMask       1.00      1.00      1.00       777\n",
            " WithoutMask       1.00      1.00      1.00       660\n",
            "\n",
            "    accuracy                           1.00      1437\n",
            "   macro avg       1.00      1.00      1.00      1437\n",
            "weighted avg       1.00      1.00      1.00      1437\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM4xNXVg2v1q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a4b0818-fcd9-4a73-f4f6-8decdf22c355"
      },
      "source": [
        "# serialize the model to disk\n",
        "print(\"[INFO] saving model...\")\n",
        "f = open(MODEL_PATH, \"wb\")\n",
        "f.write(pickle.dumps(model))\n",
        "f.close()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] saving model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Cr6GZM6GCkZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b1d779ad-3055-44f8-f89c-8ed8b8d18dfa"
      },
      "source": [
        "print(testY)\n",
        "print(preds)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1' '0' '0' ... '0' '1' '1']\n",
            "['1' '0' '0' ... '0' '1' '1']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}